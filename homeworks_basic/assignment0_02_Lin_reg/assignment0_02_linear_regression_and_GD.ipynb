{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqEpGyyyGE1Z",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "## Solving the linear regression problem with gradient descent\n",
    "\n",
    "Today we rewise the linear regression algorithm and it's gradient solution.\n",
    "\n",
    "Your main goal will be to __derive and implement the gradient of MSE, MAE, L1 and L2 regularization terms__ respectively in general __vector form__ (when both single observation $\\mathbf{x}_i$ and corresponding target value $\\mathbf{y}_i$ are vectors).\n",
    "\n",
    "This techniques will be useful later in Deep Learning module of our course as well.\n",
    "\n",
    "We will work with [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) subset, which have been preprocessed for your convenience."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 1,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you are using Google Colab, uncomment the next lines to download `loss_and_derivatives.py` and `boston_subset.json`\\nYou can open and change downloaded `.py` files in Colab using the \"Files\" sidebar on the left.\\n'"
      ]
     },
<<<<<<< HEAD
     "execution_count": 2,
=======
     "execution_count": 1,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "If you are using Google Colab, uncomment the next lines to download `loss_and_derivatives.py` and `boston_subset.json`\n",
    "You can open and change downloaded `.py` files in Colab using the \"Files\" sidebar on the left.\n",
    "'''\n",
    "# !wget https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/homeworks_basic/assignment0_02_Lin_reg/loss_and_derivatives.py\n",
    "# !wget https://raw.githubusercontent.com/ml-mipt/ml-mipt/basic_s20/homeworks_basic/assignment0_02_Lin_reg/boston_subset.json"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 2,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lQUR89nGE1f"
   },
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 3,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGf3ShTNGE1q"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('boston_subset.json', 'r') as iofile:\n",
    "    dataset = json.load(iofile)\n",
    "feature_matrix = np.array(dataset['data'])\n",
    "targets = np.array(dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BIUU1cOZGE10"
   },
   "source": [
    "## Warming up: matrix differentiation\n",
    "_You will meet these questions later in Labs as well, so we highly recommend to answer them right here._\n",
    "\n",
    "Credits: this theoretical part is copied from [YSDA Practical_DL course](https://github.com/yandexdataschool/Practical_DL/tree/spring2019/homework01) homework01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvrZt_xNGE12"
   },
   "source": [
    "Since it easy to google every task please please please try to understand what's going on. The \"just answer\" thing will not be  counted, make sure to present derivation of your solution. It is absolutely OK if you will find an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ty4m156yGE15"
   },
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
    "[4](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8StFOCFGE17"
   },
   "source": [
    "#### Inline question 1\n",
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = 2 x\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qtnNCP4JGE19"
   },
   "source": [
    "#### Inline question 2\n",
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = B^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWfcC7_dGE2A"
   },
   "source": [
    "###### Inline question 3\n",
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = Ac\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} = x c^{T}\n",
    "$$\n",
    "\n",
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbBc_5FhGE2B"
   },
   "source": [
    "## Loss functions and derivatives implementation\n",
    "You will need to implement the methods from `loss_and_derivatives.py` to go further.\n",
    "__In this assignment we ignore the bias term__, so the linear model takes simple form of \n",
    "$$\n",
    "\\hat{\\mathbf{y}} = XW\n",
    "$$\n",
    "where no extra column of 1s is added to the $X$ matrix.\n",
    "\n",
    "Implement the loss functions, regularization terms and their derivatives with reference to (w.r.t.) weight matrix. \n",
    "\n",
    "__Once again, you can assume that linear model is not required for bias term for now. The dataset is preprocessed for this case.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-CX9dTLGE1y"
   },
   "source": [
    "Autoreload is a great stuff, but sometimes it does not work as intended. The code below aims to fix that. __Do not forget to save your changes in the `.py` file before reloading the desired functions.__"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtELlRTOGE2E",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# This dirty hack might help if the autoreload has failed for some reason\n",
    "try:\n",
    "    del LossAndDerivatives\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from loss_and_derivatives import LossAndDerivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mention, that in this case we compute the __MSE__ and __MAE__ for vector __y__. In the reference implementation we are averaging the error along the __y__ dimentionality as well.\n",
    "\n",
    "E.g. for residuals vector $[1., 1., 1., 1.]$ the averaged error value will be $\\frac{1}{4}(1. + 1. + 1. + 1.)$ \n",
    "\n",
    "This may be needed to get the desired mutliplier for loss functions derivatives. You also can refer to the `.mse` method implementation, which is already available in the `loss_and_derivatives.py`."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 16,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71VCxUwHGE2L"
   },
   "outputs": [],
   "source": [
    "w = np.array([1., 1.])\n",
    "x_n, y_n = feature_matrix, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMN81aYyGE2T"
   },
   "source": [
    "Here come several asserts to check yourself:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 17,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKUYnPWuGE2V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((406, 2), (406, 4), (2, 4))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([1., 1.])\n",
    "x_n, y_n = feature_matrix, targets\n",
    "\n",
    "# Repeating data to make everything multi-dimentional\n",
    "w = np.vstack([w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]).T\n",
    "y_n = np.hstack([y_n[:, None], 2*y_n[:, None], 3*y_n[:, None], 4*y_n[:, None]])\n",
<<<<<<< HEAD
    "x_n.shape, y_n.shape, w.shape"
=======
    "np.prod?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, (2, 4))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.prod(w.shape),w.shape"
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 19,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1344,
     "status": "error",
     "timestamp": 1582397124081,
     "user": {
      "displayName": "Victor Yacovlev",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDahDnBQR6_kQQX4xt7llKTI0xt2Z802bvVR4MrqA=s64",
      "userId": "11689260236152306260"
     },
     "user_tz": -180
    },
    "id": "UtkO4hWYGE2c",
    "outputId": "cb0b99a8-2db4-4873-dfd8-741b52db29f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE derivative:\n",
      "[[ 7.32890068 12.88731311 18.82128365 23.97731238]\n",
      " [ 9.55674399 17.05397661 24.98807528 32.01723714]] \n",
      "\n",
      "L2 reg derivative:\n",
      "[[2.54 2.44 2.9  2.2 ]\n",
      " [2.54 2.44 2.9  2.2 ]]\n"
     ]
    }
   ],
   "source": [
    "reference_mse_derivative = np.array([\n",
    "    [ 7.32890068, 12.88731311, 18.82128365, 23.97731238],\n",
    "    [ 9.55674399, 17.05397661, 24.98807528, 32.01723714]\n",
    "])\n",
    "reference_l2_reg_derivative = np.array([\n",
    "    [2.54, 2.44, 2.9 , 2.2 ],\n",
    "    [2.54, 2.44, 2.9 , 2.2 ]\n",
    "])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mse_derivative,\n",
    "    LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), 'Something wrong with MSE derivative'\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l2_reg_derivative,\n",
    "    LossAndDerivatives.l2_reg_derivative(w), rtol=1e-3\n",
    "), 'Something wrong with L2 reg derivative'\n",
    "\n",
    "print(\n",
    "    'MSE derivative:\\n{} \\n\\nL2 reg derivative:\\n{}'.format(\n",
    "        LossAndDerivatives.mse_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l2_reg_derivative(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 20,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Something wrong with MAE derivative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-1ed0980c9b77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mreference_mae_derivative\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mLossAndDerivatives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmae_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m ), 'Something wrong with MAE derivative'\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m assert np.allclose(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Something wrong with MAE derivative"
     ]
    }
   ],
   "source": [
    "reference_mae_derivative = np.array([\n",
    "    [0.19708867, 0.19621798, 0.19621798, 0.19572906],\n",
    "    [0.25574138, 0.25524507, 0.25524507, 0.25406404]\n",
    "])\n",
    "reference_l1_reg_derivative = np.array([\n",
    "    [1., 1., 1., 1.],\n",
    "    [1., 1., 1., 1.]\n",
    "])\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_mae_derivative,\n",
    "    LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3\n",
    "), 'Something wrong with MAE derivative'\n",
    "\n",
    "assert np.allclose(\n",
    "    reference_l1_reg_derivative,\n",
    "    LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3\n",
    "), 'Something wrong with L1 reg derivative'\n",
    "\n",
    "print(\n",
    "    'MAE derivative:\\n{} \\n\\nL1 reg derivative:\\n{}'.format(\n",
    "        LossAndDerivatives.mae_derivative(x_n, y_n, w),\n",
    "        LossAndDerivatives.l1_reg_derivative(w))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 21,
>>>>>>> 1056e19bfe492d2130721a7f8634baafe8c1d997
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "On6aSWuIGE21"
   },
   "outputs": [],
   "source": [
    "def get_w_by_grad(X, Y, w_0, loss_mode='mse', reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05):\n",
    "    if loss_mode == 'mse':\n",
    "        loss_function = LossAndDerivatives.mse\n",
    "        loss_derivative = LossAndDerivatives.mse_derivative\n",
    "    elif loss_mode == 'mae':\n",
    "        loss_function = LossAndDerivatives.mae\n",
    "        loss_derivative = LossAndDerivatives.mae_derivative\n",
    "    else:\n",
    "        raise ValueError('Unknown loss function. Available loss functions: `mse`, `mae`')\n",
    "    \n",
    "    if reg_mode is None:\n",
    "        reg_function = LossAndDerivatives.no_reg\n",
    "        reg_derivative = LossAndDerivatives.no_reg_derivative # lambda w: np.zeros_like(w)\n",
    "    elif reg_mode == 'l2':\n",
    "        reg_function = LossAndDerivatives.l2_reg\n",
    "        reg_derivative = LossAndDerivatives.l2_reg_derivative\n",
    "    elif reg_mode == 'l1':\n",
    "        reg_function = LossAndDerivatives.l1_reg\n",
    "        reg_derivative = LossAndDerivatives.l1_reg_derivative\n",
    "    else:\n",
    "        raise ValueError('Unknown regularization mode. Available modes: `l1`, `l2`, None')\n",
    "    \n",
    "    \n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n",
    "        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm > 5.:\n",
    "            gradient = gradient / gradient_norm * 5.\n",
    "        w -= lr * gradient\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            print('Step={}, loss={},\\ngradient values={}\\n'.format(i, empirical_risk, gradient))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1pyDIyqGE25"
   },
   "outputs": [],
   "source": [
    "# Initial weight matrix\n",
    "w = np.ones((2,1), dtype=float)\n",
    "y_n = targets[:, None] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erTRQiAFGE29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=0, loss=231.28353984777308,\n",
      "gradient values=[[3.03360308]\n",
      " [3.97457575]]\n",
      "\n",
      "Step=25, loss=58.512903511682715,\n",
      "gradient values=[[2.28551977]\n",
      " [4.44706638]]\n",
      "\n",
      "Step=50, loss=48.29584498872882,\n",
      "gradient values=[[-0.89558132]\n",
      " [ 0.76425616]]\n",
      "\n",
      "Step=75, loss=47.292783042717005,\n",
      "gradient values=[[-0.48111511]\n",
      " [ 0.40907079]]\n",
      "\n",
      "Step=100, loss=47.00419092029711,\n",
      "gradient values=[[-0.25806412]\n",
      " [ 0.21942022]]\n",
      "\n",
      "Step=125, loss=46.921159712801064,\n",
      "gradient values=[[-0.1384223 ]\n",
      " [ 0.11769421]]\n",
      "\n",
      "Step=150, loss=46.897270698227686,\n",
      "gradient values=[[-0.07424796]\n",
      " [ 0.06312967]]\n",
      "\n",
      "Step=175, loss=46.890397559386315,\n",
      "gradient values=[[-0.03982566]\n",
      " [ 0.03386195]]\n",
      "\n",
      "Step=200, loss=46.88842007984702,\n",
      "gradient values=[[-0.02136197]\n",
      " [ 0.01816312]]\n",
      "\n",
      "Step=225, loss=46.88785113668749,\n",
      "gradient values=[[-0.01145829]\n",
      " [ 0.00974247]]\n",
      "\n",
      "Step=250, loss=46.88768744532566,\n",
      "gradient values=[[-0.00614608]\n",
      " [ 0.00522573]]\n",
      "\n",
      "Step=275, loss=46.88764034947525,\n",
      "gradient values=[[-0.00329668]\n",
      " [ 0.00280302]]\n",
      "\n",
      "Step=300, loss=46.88762679946807,\n",
      "gradient values=[[-0.0017683]\n",
      " [ 0.0015035]]\n",
      "\n",
      "Step=325, loss=46.88762290097823,\n",
      "gradient values=[[-0.00094849]\n",
      " [ 0.00080646]]\n",
      "\n",
      "Step=350, loss=46.88762177933875,\n",
      "gradient values=[[-0.00050876]\n",
      " [ 0.00043257]]\n",
      "\n",
      "Step=375, loss=46.88762145663043,\n",
      "gradient values=[[-0.00027289]\n",
      " [ 0.00023203]]\n",
      "\n",
      "Step=400, loss=46.887621363783595,\n",
      "gradient values=[[-0.00014638]\n",
      " [ 0.00012446]]\n",
      "\n",
      "Step=425, loss=46.88762133707051,\n",
      "gradient values=[[-7.85140451e-05]\n",
      " [ 6.67569343e-05]]\n",
      "\n",
      "Step=450, loss=46.88762132938486,\n",
      "gradient values=[[-4.21139322e-05]\n",
      " [ 3.58075680e-05]]\n",
      "\n",
      "Step=475, loss=46.887621327173605,\n",
      "gradient values=[[-2.25893760e-05]\n",
      " [ 1.92067227e-05]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_grad = get_w_by_grad(x_n, y_n, w, loss_mode='mse', reg_mode='l2', n_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with `sklearn`\n",
    "Finally, let's compare our model with `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn linear regression implementation delivers MSE = 42.53541245128315\n"
     ]
    }
   ],
   "source": [
    "lr = Ridge(alpha=0.05)\n",
    "lr.fit(x_n, y_n)\n",
    "print('sklearn linear regression implementation delivers MSE = {}'.format(np.mean((lr.predict(x_n) - y_n)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gse1m4nyGE3C"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_n[:, -1], y_n[:, -1], color=\"c\")\n",
    "plt.scatter(x_n[:, -1], x_n.dot(w_grad)[:, -1], color='red', label='Handwritten linear regression', linewidth=3)\n",
    "plt.scatter(x_n[:, -1], lr.predict(x_n), color='blue', label='sklearn Ridge')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the solutions may look like a bit different, remember, that handwritten linear regression was unable to fit the bias term, it was equal to $0$ by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GgeWdBmGE3H"
   },
   "source": [
    "### Submit your work\n",
    "To submit your work you need to log into Yandex contest (link will be provided later) and upload the `loss_and_derivatives.py` file for the corresponding problem."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment0_02_linear_regression_and_gradient_descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
